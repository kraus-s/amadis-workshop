{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We need to get everything set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install https://huggingface.co/easyh/de_fnhd_nerdh/resolve/main/de_fnhd_nerdh-any-py3-none-any.whl\n",
    "!python -m spacy download \"de_core_news_lg\"\n",
    "%pip install pyldavis\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim_models as gsv\n",
    "from constants import STOPWORDS as stops\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from helpers import leven_worker, pre_cleaning\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = Path(\"./input/\").rglob(\"*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = {}\n",
    "\n",
    "for i in input_files:\n",
    "    with open(i, \"r+\") as f:\n",
    "        txt = f.read()\n",
    "    txt = pre_cleaning(txt)\n",
    "    book_name = str(i).split(\"/\")[-1]\n",
    "    book_name = book_name.split(\".\")[0]\n",
    "    full_corpus[book_name] = txt\n",
    "print(len(full_corpus))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 1: Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_chapters = {}\n",
    "\n",
    "for k, v in full_corpus.items():\n",
    "    chaps = v.split(\"Capitel\")\n",
    "    chap_num = 0\n",
    "    for i in chaps:\n",
    "        curr_chap = f\"{k}-{chap_num}-Kap\"\n",
    "        by_chapters[curr_chap] = i\n",
    "        chap_num += 1\n",
    "print(len(by_chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list: list[list[str]] = []\n",
    "for i in nlp.pipe(by_chapters.values(), n_process=7):\n",
    "    itoks = [token.lemma_.lower() for token in i if token.lower_ not in stops and \"\\n\" not in token.lower_ and not token.is_stop]\n",
    "    docs_list.append(itoks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = gensim.models.Phrases(docs_list, min_count=5, threshold=2)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[docs_list], threshold=2)\n",
    "bigrams = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigrams = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "docs_list = trigrams[bigrams[docs_list]]\n",
    "corpus_dictionary = gensim.corpora.Dictionary(docs_list)\n",
    "analysis_corpus = [corpus_dictionary.doc2bow(i) for i in docs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "score = []\n",
    "for i in range(1, 40,1):\n",
    "   lda_model = LdaMulticore(corpus=analysis_corpus,\n",
    "               id2word=corpus_dictionary,\n",
    "               iterations=75,\n",
    "               num_topics=i,\n",
    "               workers = 7,\n",
    "               passes=12,\n",
    "               random_state=100)\n",
    "   cm = CoherenceModel(model=lda_model,\n",
    "                       corpus=analysis_corpus,\n",
    "                       dictionary=corpus_dictionary,\n",
    "                       coherence='c_v',\n",
    "                       texts=docs_list)\n",
    "   topics.append(i)\n",
    "   score.append(cm.get_coherence())\n",
    "_=plt.plot(topics, score)\n",
    "_=plt.xlabel('Number of Topics')\n",
    "_=plt.ylabel('Coherence Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = LdaMulticore(corpus=analysis_corpus,\n",
    "                                              id2word=corpus_dictionary,\n",
    "                                              iterations=75,\n",
    "                                              num_topics=25,\n",
    "                                              workers = 7,\n",
    "                                              passes=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = gsv.prepare(lda_model, analysis_corpus, corpus_dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concordances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchword = \"könig\"\n",
    "locations: list[tuple[int, int]] = []\n",
    "for doc in enumerate(docs_list):\n",
    "    doc_no, doc_toks = doc\n",
    "    for toks in enumerate(doc_toks):\n",
    "        tok_no, tok = toks\n",
    "        if tok == searchword:\n",
    "            locations.append((doc_no, tok_no))\n",
    "\n",
    "for location in locations:\n",
    "    doc_loc = location[0]\n",
    "    tok_scope_from = location[1]-5\n",
    "    tok_scope_to = location[1]+5\n",
    "    print(docs_list[doc_loc][tok_scope_from:tok_scope_to])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequenzanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchword = \"abend\"\n",
    "count = 0\n",
    "for doc in docs_list:\n",
    "    for tok in doc:\n",
    "        if tok == searchword:\n",
    "            count += 1\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 2: Stilometrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_dict: dict[str, list[str]] = {\"Montalvo\": ['4__Buch', '1__Buch'], \"Silva\": ['7__Buch', '10__Buch'], \"Unknown\": [\"22__Buch\"], \"Roseo\": [\"19__Buch\"]}\n",
    "translator_dict: dict[str, str] = {}\n",
    "known = [\"Montalvo\", \"Silva\", \"Roseo\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic: Wort- und Satzlängen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wortlängen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_books_clean: dict[str, list[str]] = {}\n",
    "for k, v in full_corpus.items():\n",
    "    i = nlp(v)\n",
    "    itoks = [token.lemma_.lower() for token in i if token.lower_ not in stops and \"\\n\" not in token.lower_ and not token.is_stop]\n",
    "    whole_books_clean[k] = itoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_by_book = {}\n",
    "for k, v in whole_books_clean.items():\n",
    "    token_lengths = [len(token) for token in v]\n",
    "    freq_dist = Counter(token_lengths)\n",
    "    length_by_book[k] = freq_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in length_by_book.items():\n",
    "    plt.bar(v.keys(), v.values())\n",
    "    plt.title(f\"{k}\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satzlängen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_by_sents: dict[str, list[str]] = {}\n",
    "for k, v in full_corpus.items():\n",
    "    i = nlp(v)\n",
    "    sents = [sent.text for sent in i.sents]\n",
    "    books_by_sents[k] = sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_by_book = {}\n",
    "for k, v in books_by_sents.items():\n",
    "    sent_lengths = [len(sent.split()) for sent in v]\n",
    "    freq_dist = Counter(sent_lengths)\n",
    "    length_by_book[k] = freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in length_by_book.items():\n",
    "    plt.bar(v.keys(), v.values())\n",
    "    plt.title(f\"{k}\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Burrows Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_books_burrows: dict[str, list[str]] = {}\n",
    "for k, v in full_corpus.items():\n",
    "    i = nlp(v)\n",
    "    itoks = [token.lemma_.lower() for token in i if \"\\n\" not in token.lower_ and \"--\" not in token.lower_]\n",
    "    whole_books_burrows[k] = itoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_corpus = []\n",
    "for k, v in whole_books_burrows.items():\n",
    "    complete_corpus.extend(v)\n",
    "len(complete_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_corpus_freq_dist = list(nltk.FreqDist(complete_corpus).most_common(30))\n",
    "whole_corpus_freq_dist[ :10 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [word for word,freq in whole_corpus_freq_dist]\n",
    "feature_freqs = {}\n",
    "\n",
    "for i in known:\n",
    "    feature_freqs[i] = {}\n",
    "    personal_complete = []\n",
    "\n",
    "    for ii in author_dict[i]:\n",
    "        personal_complete.extend(whole_books_burrows[ii])\n",
    "\n",
    "    token_count = len(personal_complete)\n",
    "    for feature in features:\n",
    "        presence = personal_complete.count(feature)\n",
    "        feature_freqs[i][feature] = presence / token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_features = {}\n",
    "\n",
    "for feature in features:\n",
    "    corpus_features[feature] = {}\n",
    "\n",
    "    feature_avg = 0\n",
    "    for person in known:\n",
    "        feature_avg += feature_freqs[person][feature]\n",
    "    feature_avg /= len(known)\n",
    "    corpus_features[feature][\"Mean\"] = feature_avg\n",
    "\n",
    "    feature_stdev = 0\n",
    "    for person in known:\n",
    "        diff = feature_freqs[person][feature] - corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev += diff*diff\n",
    "    feature_stdev /= (len(known) - 1)\n",
    "    feature_stdev = math.sqrt(feature_stdev)\n",
    "    corpus_features[feature][\"StdDev\"] = feature_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_zscores = {}\n",
    "for person in known:\n",
    "    feature_zscores[person] = {}\n",
    "    for feature in features:\n",
    "\n",
    "        feature_val = feature_freqs[person][feature]\n",
    "        feature_mean = corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev = corpus_features[feature][\"StdDev\"]\n",
    "        feature_zscores[person][feature] = ((feature_val-feature_mean) /\n",
    "                                            feature_stdev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in author_dict[\"Unknown\"]:\n",
    "    unkown_text = whole_books_burrows[i]\n",
    "\n",
    "    unkown_count = len(unkown_text)\n",
    "    unkown_freqs = {}\n",
    "    for feature in features:\n",
    "        presence = unkown_text.count(feature)\n",
    "        unkown_freqs[feature] = presence / unkown_count\n",
    "    \n",
    "    unkown_zscores = {}\n",
    "    for feature in features:\n",
    "        feature_val = unkown_freqs[feature]\n",
    "        feature_mean = corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev = corpus_features[feature][\"StdDev\"]\n",
    "        unkown_zscores[feature] = (feature_val - feature_mean) / feature_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in known:\n",
    "    delta = 0\n",
    "    for feature in features:\n",
    "        delta += math.fabs((unkown_zscores[feature] -\n",
    "                            feature_zscores[person][feature]))\n",
    "    delta /= len(features)\n",
    "    print(f\"Delta score for candidate {person} is {delta}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Text reuse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple trick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = [sent for sents in books_by_sents.values() for sent in sents if len(sent.split()) > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reoccurences = Counter(all_sents)\n",
    "most_freq_sents = []\n",
    "most_freq_counts = []\n",
    "for sent in reoccurences:\n",
    "    if reoccurences[sent] > 1:\n",
    "        most_freq_sents.append(sent)\n",
    "        most_freq_counts.append(int(reoccurences[sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"Sentence\": most_freq_sents, \"Count\": most_freq_counts}\n",
    "freqs_df = pd.DataFrame(df_dict)\n",
    "freqs_df.sort_values(by=[\"Count\"], ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_combinations = itertools.combinations(all_sents[:6000], 2)\n",
    "matches_dict = {}\n",
    "matches_list = []\n",
    "for i in leven_worker(sent_combinations):\n",
    "    matches_dict[i[0]] = i[1]\n",
    "    matches_list.append(i[0])\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matches_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a414dd39a60ba325d0b1bb47a9fb3b86783498faa6b177c5f1e06bfa16726512"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
